{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ox2-CDXSOal"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T_OHJNZa4OlN"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QVESjRva-Ls1"
   },
   "outputs": [],
   "source": [
    "os.sys.path.append('/content/gdrive/path/to/module_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jz--oRm-Y8_"
   },
   "outputs": [],
   "source": [
    "os.sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VsbCk5Oe1DI3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from docopt import docopt\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "from nmt import Hypothesis, NMT\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from utils import read_corpus, batch_iter\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pCMEQ-HS-F5W"
   },
   "outputs": [],
   "source": [
    "def evaluate_ppl(model, dev_data, batch_size=32):\n",
    "    \"\"\" Evaluate perplexity on dev sentences\n",
    "    @param model (NMT): NMT Model\n",
    "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (batch size)\n",
    "    @returns ppl (perplixty on dev sentences)\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    cum_loss = 0.\n",
    "    cum_tgt_words = 0.\n",
    "    \n",
    "    # no_grad() signals backend to throw away all gradients\n",
    "    with torch.no_grad():\n",
    "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
    "            loss = -model(src_sents, tgt_sents).sum()\n",
    "            \n",
    "            cum_loss += loss.item()\n",
    "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents) # omitting leading '<s>'\n",
    "            cum_tgt_words += tgt_word_num_to_predict\n",
    "            \n",
    "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
    "        \n",
    "    if was_training:\n",
    "        model.train()\n",
    "        \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yhv4c505-c-1"
   },
   "outputs": [],
   "source": [
    "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
    "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
    "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
    "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
    "    @returns bleu_score: corpus-level BLEU score\n",
    "    \"\"\"\n",
    "    if references[0][0] == '<s>':\n",
    "        references = [ref[1:-1] for ref in references]\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
    "                             [hyp.value for hyp in hypotheses])\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QKhDahRs-g6E"
   },
   "outputs": [],
   "source": [
    "def decode(args: Dict[str, str]):\n",
    "    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n",
    "    If the target gold-standard sentences are given, the function also computes\n",
    "    corpus-level BLEU score.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "    \n",
    "    print('load test source sentences from [{}]'.format(args['test_src']), file=sys.stderr)\n",
    "    test_data_src = read_corpus(args['test_src'], source='src')\n",
    "    if args['test_tgt']:\n",
    "        print(\"load test target sentences from [{}]\".format(args['test_tgt']), file=sys.stderr)\n",
    "        test_data_tgt = read_corpus(args['test_tgt'], source='tgt')\n",
    "        \n",
    "    print(\"load model from {}\".format(args['model_path']), file=sys.stderr)\n",
    "    model = NMT.load(args['model_path'])\n",
    "    \n",
    "    if args['cuda']:\n",
    "        model = model.to(torch.device(\"cuda\"))\n",
    "    \n",
    "    hypotheses = beam_search(model, test_data_src,\n",
    "                             beam_size=int(args['beam_size']),\n",
    "                             max_decoding_time_step=int(args['max_decoding_time_step']))\n",
    "    \n",
    "    if args['test_tgt']:\n",
    "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
    "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
    "        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
    "        \n",
    "    if not os.path.exists(args['output_dir']):\n",
    "      os.makedirs(args['output_dir'])\n",
    "\n",
    "    with open(os.path.join(args['output_dir'], args['output_file']), 'w') as f:\n",
    "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
    "            top_hyp = hyps[0]\n",
    "            hyp_sent = ' '.join(top_hyp.value)\n",
    "            f.write(hyp_sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gfSzFY6--jb0"
   },
   "outputs": [],
   "source": [
    "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
    "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
    "    @param model (NMT): NMT Model\n",
    "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
    "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
    "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
    "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    hypotheses = []\n",
    "    with torch.no_grad():\n",
    "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
    "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
    "            \n",
    "            hypotheses.append(example_hyps)\n",
    "            \n",
    "    if was_training: model.train(was_training)\n",
    "        \n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V4SrHzGI-lUU"
   },
   "outputs": [],
   "source": [
    "args = dict()\n",
    "\n",
    "args['train_src'] = \"/content/gdrive/path/to/data/train.de-en.de.wmixerprep\"\n",
    "args['train_tgt'] = \"/content/gdrive/path/to/data/train.de-en.en.wmixerprep\"\n",
    "args['dev_src'] = \"/content/gdrive/path/to/data/valid.de-en.de\"\n",
    "args['dev_tgt'] = \"/content/gdrive/path/to/data/valid.de-en.en\"\n",
    "args['test_src'] = '/content/gdrive/path/to/data/test.de-en.de'\n",
    "args['test_tgt'] = '/content/gdrive/path/to/data/test.de-en.en'\n",
    "args['vocab'] = \"/content/gdrive/path/to/data/vocab.json\"\n",
    "args['model_path'] = \"/content/gdrive/path/to/checkpoint/epoch_iteration_model.bin\"\n",
    "args['output_dir'] = '/content/gdrive/path/to/output_dir''\n",
    "\n",
    "args['output_file'] = 'output.txt'\n",
    "args['seed'] = 0\n",
    "args['batch_size'] = 32\n",
    "args['embed_size'] = 256\n",
    "args['hidden_size'] = 256\n",
    "args['clip_grad'] = 5.0\n",
    "args['log_every'] = 10\n",
    "args['max_epoch'] = 30\n",
    "args['patience'] = 5\n",
    "args['max_num_trial'] = 5\n",
    "args['lr_decay'] = 0.5\n",
    "args['beam_size'] = 5\n",
    "args['lr'] = 0.001\n",
    "args['uniform_init'] = 0.1\n",
    "args['save_to'] = '/content/gdrive/path/to/checkpoint'\n",
    "args['valid_niter'] = 100\n",
    "args['dropout'] = 0.3\n",
    "args['max_decoding_time_step'] = 70\n",
    "args['cuda'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iQbdFeC_Gb5S"
   },
   "outputs": [],
   "source": [
    "# seed the random number generators\n",
    "seed = int(args['seed'])\n",
    "torch.manual_seed(seed)\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed * 13 // 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "122Y40e4HH-J"
   },
   "outputs": [],
   "source": [
    "decode(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyNnvzKpHPAp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "decode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
